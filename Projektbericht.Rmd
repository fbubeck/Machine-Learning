---
title: "Projektbericht"
author: "Ferdinand Bubeck und Johannes Bubeck"
date: "19.01.2021"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Data Exploration
```{r message=FALSE, warning=FALSE}
#import libraries
library(readr)
library(dplyr)
library(ggplot2)
library(tidyverse)

#import CSVs
datTest <- read_csv("aug_test.csv")

datTrain <- read_csv("aug_train.csv")

#join of Train & Test Data to have full Dataset for Exploration
dat <- full_join(datTest, datTrain) 

#first Impressions of Data
head(dat)
tail(dat)
    
summary(dat)

dim(dat)

str(dat)

#count NAs in column gender
sum(is.na(dat$gender))

#Plot zu Geschlechterverteilung
ggplot(data=dat, aes(x=gender))+
  geom_bar(color='blue', fill='blue', alpha=.5)+
  theme_classic()+
  labs(title="Geschlechterverteilung der Teilnehmer")+
  xlab("Geschlecht")+
  ylab("Anzahl")

#Plot der Abschlüsse
ggplot(data=dat, aes(x=education_level))+
  geom_bar(color='orange', fill='orange', alpha=.5)+
  coord_flip()+
  theme_classic()+
  labs(title="Verteilung der Abschlüsse bei den Teilnehmer")+
  xlab("Art des Abschlusses")+
  ylab("Anzahl")

#Plot Anzahl der Trainngsstunden und Abschluss (Boxplot)
ggplot(data=dat, aes(x=education_level, y=training_hours))+
  geom_boxplot()+
  theme_classic()+
  labs(title="Abschluss und Anzahl der Trainingsstunden")+
  xlab("Art des Abschlusses")+
  ylab("Anzahl Trainingsstunden")

#Plot Anzahl der Trainngsstunden und Abschluss (Point/Jitter)
ggplot(data=dat, aes(x=education_level, y=training_hours))+
  geom_jitter(color='black', alpha=.3, size=.9)+
  theme_classic()+
  labs(title="Abschluss und Anzahl der Trainingsstunden")+
  xlab("Art des Abschlusses")+
  ylab("Anzahl Trainingsstunden")

#ersetze < und > durch feste numerische Werte um typecast durchzuführen
dat$experience <- sub("<1", "0", dat$experience)
dat$experience <- sub(">20", "21", dat$experience)
dat$experience <- as.numeric(dat$experience)

dat %>% 
  select(enrolled_university, experience) %>%
  filter(!is.na(enrolled_university),!is.na(experience)) %>%
  ggplot(data=., aes(x=enrolled_university, y=experience))+
  geom_boxplot(fill="grey", alpha=.7)+
  theme_classic()+
  labs(title="Berufserfahrung und Immatrikulationsstatus")+
  xlab("Immatrikulationsstatus")+
  ylab("Berufserfahrung")


###
ggplot()+
  geom_histogram(data=subset(dat, gender == "Male"), aes(x=training_hours), fill="blue", alpha=.4)+
  geom_histogram(data=subset(dat, gender == "Female"), aes(x=training_hours), fill="red", alpha=.4)+
  geom_histogram(data=subset(dat, gender == "Other"), aes(x=training_hours), fill="grey", alpha=.4)+
  theme_classic()+
  labs(title="Histogramm Anzahl Trainingsstunden und Geschlecht")+
  xlab("Anzahl Trainingsstunden")+
  ylab("Count")

dat %>%
  select(training_hours, target) %>%
  filter(!is.na(training_hours),!is.na(target)) %>%
  ggplot(data=., aes(x = training_hours)) +
    geom_histogram(bins = 25, color = "black", fill = "violet", alpha=.7) +
    theme_light() +
    facet_wrap(vars(target))+
    labs(title="Histogramm Trainingsstunden nach target")+
    xlab("Anzahl Trainingsstunden")+
    ylab("Count")

ggplot(data=dat, aes(x=as.factor(education_level)))+
  geom_bar(data=dat, aes(fill=as.factor(education_level)))+
  facet_wrap(vars(relevent_experience))+
  theme_light()+
  theme(legend.position = "none")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  labs(title="Relevante Berufserfahrung in Bezug auf Schulabaschluss")+
  xlab("Schulabschluss")

dat %>% 
  select(major_discipline, education_level) %>%
  filter(!is.na(major_discipline),!is.na(education_level)) %>%
  ggplot(data=.,aes(x=education_level))+
    geom_bar(aes(fill=education_level))+
    facet_wrap(vars(major_discipline))+
    theme_light()+
    theme(legend.position = "none")+
    theme(axis.text.x = element_text(angle = 45, hjust = 1))+
    labs(title="Schulabschluss in Bezug auf akademische Disziplin", subtitle = "log2 y-Achse")+
    xlab("akademische Disziplin")+
    scale_y_continuous(trans = 'log2')

dat %>% 
  select(city_development_index, training_hours)%>%
  mutate(city_development_index = fct_lump(as.factor(city_development_index), n = 20)) %>%
  filter(!is.na(city_development_index)) %>%
  ggplot(data= ., aes(x=city_development_index, y=training_hours, fill = city_development_index))+
    geom_col()+
    theme_light()+
    theme(axis.text.x = element_text(angle = 45, hjust = 1))+
    theme(legend.position = "none")+
    labs(title="Stadtentwicklungsindex und Trainingsstunden", subtitle = "top 20 Stadtentwicklungsindexe")+
    xlab("Stadtentwicklungsindex")+
    ylab("Trainingsstunden")
  

```

```{r message=FALSE, warning=FALSE}
library(caret)

#Dealing with NAs
#detect variables with more than 5% (threshold) of NAs
detectNA <- function(x){sum(is.na(x))/length(x)*100}
apply(datTrain, 2, detectNA)


#Data Preparation
datTrain_casted <- datTrain %>% 
  select(-enrollee_id, -city) %>%
  mutate(city_development_index = as.numeric(city_development_index),
         target = as.factor(target)) %>% 
  as.data.frame() %>%
  mutate_if(is.character, as.factor) #converting all variables with type=char to factor

  

#Data Imputation
library(mice)

impute <- mice(datTrain_casted, m = 1, remove.constant = FALSE) #takes time :)
datTrain_imputed <- complete(impute)

apply(datTrain_imputed, 2, detectNA)

#Data Balancing
library(groupdata2)

target.balancing <- datTrain_imputed %>% 
  group_by(target) %>%
  summarise(no_rows = length(target))

datTrain_balanced <- upsample(datTrain_imputed, cat_col = "target")

datTrain_balanced %>% 
  group_by(target) %>%
  summarise(no_rows = length(target))

#Test / Training Split
set.seed(3024)
trainingRows <- sort(sample(nrow(datTrain_balanced), nrow(datTrain_balanced)*.7))

Train <- datTrain_balanced[trainingRows,]
Test <- datTrain_balanced[-trainingRows,]

###Decision tree
library(rpart)
library(rpart.plot)

##1st tree
set.seed(34)
dt.default <- rpart(target~ ., data=Train, cp=-1)

printcp(dt.default)
plotcp(dt.default)

#Prediction & Performance
#Train Data
train.default.pred <- predict(dt.default, newdata = Train, type = "class")
train.default.confMatrix <- table(train.default.pred, Train[, 12])
print(train.default.confMatrix)

train.default.accuracy <- sum(diag(train.default.confMatrix))/sum(train.default.confMatrix)
print(train.default.accuracy)

#Test Data
test.default.pred <- predict(dt.default, newdata = Test, type = "class")
test.default.confMatrix <- table(test.default.pred, Test[, 12])
print(test.default.confMatrix)

test.default.accuracy <- sum(diag(test.default.confMatrix))/sum(test.default.confMatrix)
print(test.default.accuracy)

##2nd tree with cp limitation
set.seed(39)
dt.fit <- rpart(target~ ., data=Train, cp=0.005)

#summary(dt.fit)

printcp(dt.fit)
plotcp(dt.fit)

#search cp with lowest Cross-Validation Error
xerror.min <- dt.fit$cptable[which.min(dt.fit$cptable[,4]),]
cp.best <- xerror.min[1]
cp.best

#Pruning with best cp
dt.pruned <- prune(dt.fit, cp=cp.best)

#summary(dt.pruned)

rpart.plot(dt.pruned)

#Prediction & Performance
#Train Data
train.pred <- predict(dt.pruned, newdata = Train, type = "class")
train.confMatrix <- table(train.pred, Train[, 12])
print(train.confMatrix)

train.accuracy <- sum(diag(train.confMatrix))/sum(train.confMatrix)
print(train.accuracy)

#Test Data
test.pred <- predict(dt.pruned, newdata = Test, type = "class")
test.confMatrix <- table(test.pred, Test[, 12])
print(test.confMatrix)

test.accuracy <- sum(diag(test.confMatrix))/sum(test.confMatrix)
print(test.accuracy)

###glm
#dummy variable

dummy_target <- as.numeric(Train[12] == 1)
dat.dummy_target <- cbind(Train,dummy_target)

set.seed(4)
glm.fit <- glm(target ~ ., family  = binomial, Train)
#summary(glm.fit)

varImp(glm.fit, scale = FALSE)

glm.fit.predicted <- predict( 
  object = glm.fit, 
  data   = Test, 
  type   = "response"
)

#glm.fit.predicted
plot(glm.fit)

plot(
  x   = dat.dummy_target$city_development_index, 
  y   = dat.dummy_target$dummy_target, 
  col = "red"
)
lines(Train$city_development_index, glm.fit.predicted, col="blue")

```


